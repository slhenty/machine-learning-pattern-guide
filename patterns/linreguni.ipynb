{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[(< index)](../Patterns.ipynb)\n",
    "## Linear Regression - univariate\n",
    "\n",
    "#### Abbreviation\n",
    "linreguni\n",
    "\n",
    "#### Architecture\n",
    "Polynomial in one variable (aka 'input feature')\n",
    "\n",
    "#### Type\n",
    "Supervised\n",
    "\n",
    "#### Use\n",
    "Real value prediction\n",
    "\n",
    "<u>_Weakness_</u>\n",
    "- underfits non-linear data (high bias)\n",
    "\n",
    "#### Characteristics\n",
    "<u>_Objective_</u>\n",
    "- Tune a polynomial that takes the input feature, and returns a predicted output value\n",
    "\n",
    "<u>_Learning algorithm_</u>\n",
    "- Gradient descent\n",
    "\n",
    "<u>_Parameters_</u>\n",
    "- polynomial co-efficients, $\\theta$\n",
    "\n",
    "<u>_Hyper-parameters_</u>\n",
    "- learning rate, alpha: $\\alpha$\n",
    "- polynomial degree / higher-order terms: $x_{j}^{p}$\n",
    "- (for multivariate:  $x_{j}^{p}x_{k}^{q}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Packages needed for the examples\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "linreg_x = np.array([[2, 3, 5]])\n",
    "linreg_y = np.array([[11, 17, 29]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<u>_Forward function / Prediction_</u>\n",
    "> $\\normalsize\n",
    "\\begin{align}\n",
    "h_{\\theta}(x) &= \\theta_0 x_0 + \\theta_1 x_1 \\\\\n",
    "  &= \\theta \\cdot x \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "> where,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta &\\text{ is a row vector of coefficients (parameters) of the polynomial } h_{\\theta}(x) \\\\\n",
    "x &\\text{ is a col vector with } x_0 \\text{ set to } 1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "> **Example linreguni-1:**\n",
    ">\n",
    "> Given,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta &= \\begin{bmatrix}\n",
    "2 & 5\n",
    "\\end{bmatrix}\n",
    ",\\;\n",
    "x = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "> Then,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta \\cdot x &= \\begin{bmatrix} (\\theta_{[0,0]} \\times x_{[0,0]}) + (\\theta_{[0,1]} \\times x_{[1,0]}) \\end{bmatrix} \\\\\n",
    "  \\;&= \\begin{bmatrix} (2 \\times 1) + (5 \\times 2) \\end{bmatrix} \\\\\n",
    "  \\;&= \\begin{bmatrix} 2 + 10 \\end{bmatrix} \\\\\n",
    "  \\;&= \\begin{bmatrix} 12 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_h_theta(theta, features):\n",
    "    \"\"\"\n",
    "    :param theta: Row vector of polynomial coefficients (i.e. parameters)\n",
    "    :param features: Col vector of features, with x_0 set to 1 (i.e. bias term)\n",
    "    :return: Estimate of y (i.e. y_hat)\n",
    "    \"\"\"\n",
    "\n",
    "    y_hat = np.dot(theta, features)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta (shape):  (1, 2)\n",
      "[[2. 5.]] \n",
      "\n",
      "x (shape):  (1, 1)\n",
      "[[2]] \n",
      "\n",
      "X (shape) with bias terms:  (2, 1)\n",
      "[[1.]\n",
      " [2.]] \n",
      "\n",
      "y_hat (shape):  (1, 1)\n",
      "[[12.]]\n"
     ]
    }
   ],
   "source": [
    "# linreguni-fw Example\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, 0:1])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "print('theta (shape): ', theta.shape)\n",
    "print(theta, '\\n')\n",
    "print('x (shape): ', x.shape)\n",
    "print(x, '\\n')\n",
    "print('X (shape) with bias terms: ', X.shape)\n",
    "print(X, '\\n')\n",
    "\n",
    "y_hat = linreg_h_theta(theta, X)\n",
    "print('y_hat (shape): ', y_hat.shape)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "_--- Vectorized (across samples) ---_\n",
    "> $\\normalsize\n",
    "\\begin{align}\n",
    "h_{\\theta}(X) &= \\theta \\cdot X \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "> where,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta &\\text{ is a row vector of coefficients (parameters) of the polynomial } h_{\\theta} \\\\\n",
    "X &\\text{ is a } n \\times m \\text{ matrix of samples} \\\\\n",
    "n &\\text{ is the number of features; for univariate, } n = 2, (x_0,\\;x_1) \\\\\n",
    "m &\\text{ is the number of samples}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "> **Example linreguni-2:**\n",
    ">\n",
    "> Given,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta = \\begin{bmatrix}\n",
    "2 & 5\n",
    "\\end{bmatrix}\n",
    ",\\;\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "2 & 3 & 5 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$\n",
    ">\n",
    "> so $X$ contains three samples (the cols of $X$)\n",
    "\n",
    "\n",
    "> Then,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta \\cdot X &= \\begin{bmatrix}\n",
    "  (\\theta_{[0,0]} \\times X_{[0,0]}) + (\\theta_{[0,1]} \\times X_{[1,0]}) &\n",
    "  (\\theta_{[0,0]} \\times X_{[0,1]}) + (\\theta_{[0,1]} \\times X_{[1,1]}) &\n",
    "  (\\theta_{[0,0]} \\times X_{[0,2]}) + (\\theta_{[0,1]} \\times X_{[1,2]})\n",
    "\\end{bmatrix} \\\\\n",
    "  \\;&= \\begin{bmatrix}\n",
    "    (2 \\times 1) + (5 \\times 2) &\n",
    "    (2 \\times 1) + (5 \\times 3) &\n",
    "    (2 \\times 1) + (5 \\times 5)\n",
    "  \\end{bmatrix} \\\\\n",
    "  \\;&= \\begin{bmatrix}\n",
    "    (2 + 10) & (2 + 15) & (2 + 25)\n",
    "  \\end{bmatrix} \\\\\n",
    "  \\;&= \\begin{bmatrix}\n",
    "    12 & 17 & 27\n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "$\n",
    ">\n",
    "> so three predictions are returned (the cols of the result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta (shape):  (1, 2)\n",
      "[[2. 5.]] \n",
      "\n",
      "x (shape):  (1, 3)\n",
      "[[2 3 5]] \n",
      "\n",
      "X (shape) with bias terms:  (2, 3)\n",
      "[[1. 1. 1.]\n",
      " [2. 3. 5.]] \n",
      "\n",
      "y_hat (shape):  (1, 3)\n",
      "[[12. 17. 27.]]\n"
     ]
    }
   ],
   "source": [
    "# lingreguni-fw_VECT example\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, :])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "print('theta (shape): ', theta.shape)\n",
    "print(theta, '\\n')\n",
    "print('x (shape): ', x.shape)\n",
    "print(x, '\\n')\n",
    "print('X (shape) with bias terms: ', X.shape)\n",
    "print(X, '\\n')\n",
    "\n",
    "y_hat = linreg_h_theta(theta, X)\n",
    "print('y_hat (shape): ', y_hat.shape)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<u>_Loss function_</u>\n",
    "> $\\normalsize\n",
    "\\begin{align}\n",
    "L_{\\theta}(\\hat{y}, y) &= \\hat{y} - y\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "> where,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\hat{y} &\\text{ is the prediction from } h_{\\theta}(x) \\\\\n",
    "y &\\text{ is the training value that } h_{\\theta}(x) \\text{ should match}\n",
    "\\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_loss_theta(y_hat, y):\n",
    "    \"\"\"\n",
    "    :param y_hat: the prediction from h_theta\n",
    "    :param y: the training label to match\n",
    "    :return: loss, or error, of the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    loss = y_hat - y\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (shape):  (1, 1)\n",
      "[[1.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linreguni-loss\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, 0:1])\n",
    "y = np.array(linreg_y[:, 0:1])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "\n",
    "y_hat = linreg_h_theta(theta, X)\n",
    "loss = linreg_loss_theta(y_hat, y)\n",
    "\n",
    "print('loss (shape): ', loss.shape)\n",
    "print(loss, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (shape):  (1, 3)\n",
      "[[ 1.  0. -2.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linreguni-loss_VECT (across samples)\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, :])\n",
    "y = np.array(linreg_y[:, :])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "\n",
    "y_hat = linreg_h_theta(theta, X)\n",
    "loss = linreg_loss_theta(y_hat, y)\n",
    "\n",
    "print('loss (shape): ', loss.shape)\n",
    "print(loss, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<u>_Cost function_</u>\n",
    "> $\\normalsize\n",
    "\\begin{align}\n",
    "    \\;& \\;&\n",
    "    \\;&\\small \\underline{Normal}&\n",
    "    \\;& \\;&\n",
    "    \\;&\\small \\underline{Vectorized} \\\\ \\\\\n",
    "J(\\theta) &=&\n",
    "    \\;&\\frac{1}{2m}\\sum\\limits_{i=1}^{m} \\left[h_{\\theta}(x^{(i)}) - y^{(i)}\\right]^2 &\n",
    "    \\;&\\Rightarrow&\n",
    "    \\;&\\frac{1}{2m} \\left[ \\left( \\theta X - Y \\right) \\cdot \\left( \\theta X - Y \\right)^{T} \\right] \\\\\n",
    "    \\;&=&\n",
    "    \\;&\\frac{1}{2m}\\sum\\limits_{i=1}^{m} L_{\\theta}(\\hat{y}^{(i)}, y^{(i)})^2 &\n",
    "    \\;&\\Rightarrow&\n",
    "    \\;&\\frac{1}{2m} \\left( L_{\\theta} \\cdot L_{\\theta}^{T} \\right) \\\\\n",
    "\\end{align}\n",
    "$\n",
    ">\n",
    "> where,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "m &\\text{ is the number of samples} \\\\\n",
    "\\\\ \\text{and} \\\\\n",
    "\\; &\\text{vectorization is across samples, } m\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_cost_from_loss(L_theta):\n",
    "    \"\"\"\n",
    "    :param L_theta: matrix (1 x m) of errors (loss)\n",
    "    :return: cost (mean squared error) to be minimized\n",
    "    \"\"\"\n",
    "\n",
    "    m = L_theta.shape[1]\n",
    "    cost = np.dot(L_theta, L_theta.T) / (2 * m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_cost_from_inputs(theta, X, Y):\n",
    "    \"\"\"\n",
    "    :param theta: row vector (1 x n) of polynomial coefficients (i.e. parameters)\n",
    "    :param X: n x m matrix of features, with x_0's set to 1 (i.e. bias terms)\n",
    "    :param Y: row vector (1 x m) of target values\n",
    "    :return: cost (mean squared error) to be minimized\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    cost = np.dot(np.dot(theta, X) - y, (np.dot(theta, X) - y).T) / (2 * m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_from_loss (shape):  (1, 1)\n",
      "[[0.83333333]] \n",
      "\n",
      "cost_from_inputs (shape):  (1, 1)\n",
      "[[0.83333333]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linreguni-cost\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, :])\n",
    "y = np.array(linreg_y[:, :])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "\n",
    "y_hat = linreg_h_theta(theta, X)\n",
    "L_theta = linreg_loss_theta(y_hat, y)\n",
    "\n",
    "cost_from_loss = linreg_cost_from_loss(L_theta)\n",
    "print('cost_from_loss (shape): ', cost_from_loss.shape)\n",
    "print(cost_from_loss, '\\n')\n",
    "\n",
    "cost_from_inputs = linreg_cost_from_inputs(theta, X, y)\n",
    "print('cost_from_inputs (shape): ', cost_from_inputs.shape)\n",
    "print(cost_from_inputs, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<u>_Backward function / Learning / Parameter update_</u>\n",
    "<br/><br/>\n",
    "> $\\normalsize\n",
    "\\begin{align}\n",
    "    \\;& \\;&\n",
    "        \\;&\\small \\underline{Normal}&\n",
    "        \\;& \\;&\n",
    "        \\;&\\small \\underline{Vectorized} \\\\ \\\\\n",
    "dJ(\\theta)\n",
    "        &=&\n",
    "        \\;&\\frac{1}{m}\\sum\\limits_{i=1}^{m} L_{\\theta}(\\hat{y}^{(i)}, y^{(i)})x_{j}^{(i)}&\n",
    "        \\;&\\Rightarrow&\n",
    "        \\;&\\frac{1}{m} \\left( L_{\\theta} \\cdot X^{T}\\right) \\\\\n",
    "    \\;&=&\n",
    "        \\;&\\frac{1}{m}\\sum\\limits_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})x_{j}^{(i)}&\n",
    "        \\;& \\;&\n",
    "        \\;& \\; \\\\\n",
    "    \\;&=&\n",
    "        \\;&\\frac{1}{m}\\sum\\limits_{i=1}^{m} \\left[h_{\\theta}(x^{(i)}) - y^{(i)}\\right]x_{j}^{(i)}&\n",
    "        \\;&\\Rightarrow&\n",
    "        \\;&\\frac{1}{m} \\; \\left[ (\\theta X - Y) \\cdot X^{T} \\right]\n",
    "\\end{align}\n",
    "$\n",
    "> <br><br><br>\n",
    "> $\\normalsize\n",
    "\\begin{align}\n",
    "    \\;& \\;&\n",
    "        \\;&\\small \\underline{Normal}&\n",
    "        \\;& \\;&\n",
    "        \\;&\\small \\underline{Vectorized} \\\\ \\\\\n",
    "\\theta_{j}\n",
    "        &:= \\theta_{j} \\; -&\n",
    "        \\;&\\alpha \\; dJ(\\theta)&\n",
    "        \\;& \\;&\n",
    "        \\;& \\; \\\\\n",
    "    \\;&:= \\theta_{j} \\; -&\n",
    "        \\;&\\boxed{\\alpha \\left( \\frac{1}{m}\\sum\\limits_{i=1}^{m} \\left[h_{\\theta}(x^{(i)}) - y^{(i)}\\right]x_{j}^{(i)} \\right) }&\n",
    "        \\;&\\Rightarrow&\n",
    "        \\;&\\boxed{\\alpha \\left( \\frac{1}{m} \\; \\left[ (\\theta X - Y) \\cdot X^{T} \\right] \\right) } \\\\\n",
    "    \\;& \\;&\n",
    "        \\; &\\small{\\text{OR}}&\n",
    "        \\;& \\;&\n",
    "        \\;& \\; \\\\\n",
    "    \\;& \\;&\n",
    "        \\;&\\boxed{\\alpha \\left( \\frac{1}{m}\\sum\\limits_{i=1}^{m} L_{\\theta}(\\hat{y}^{(i)}, y^{(i)})x_{j}^{(i)} \\right) }&\n",
    "        \\;&\\Rightarrow&\n",
    "        \\;&\\boxed{\\alpha \\left( \\frac{1}{m} \\left[ L_{\\theta} \\cdot X^{T}\\right] \\right)}\n",
    "\\end{align}\n",
    "$\n",
    ">\n",
    "> where,\n",
    ">\n",
    "> $\n",
    "\\begin{align}\n",
    "\\theta &\\text{ is a row vector of } n \\text{ parameters, i.e. polynomial coefficients} \\\\\n",
    "X &\\text{ is a } n \\times m \\text{ matrix of samples} \\\\\n",
    "Y &\\text{ is a row vector of } m \\text{ target values} \\\\\n",
    "m &\\text{ is the number of samples} \\\\\n",
    "n &\\text{ is the number of features (incl. bias terms), i.e. polynomial terms} \\\\\n",
    "i &\\text{ is the index } (1..m) \\text{ of the samples} \\\\\n",
    "j &\\text{ is the index } (1..n) \\text{ of the features, i.e. polynomial terms} \\\\\n",
    "\\alpha &\\text{ is the learning rate}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_grad_from_loss(L_theta, X):\n",
    "    \"\"\"\n",
    "    :param L_theta: row vector (1 x m) of errors (loss)\n",
    "    :param X: n x m matrix of input features\n",
    "    :return: gradient at the current theta\n",
    "    \"\"\"\n",
    "\n",
    "    m = L_theta.shape[1]\n",
    "    grad = np.dot(L_theta, X.T) / m\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_grad_from_inputs(theta, X, Y):\n",
    "    \"\"\"\n",
    "    :param theta: row vector (1 x n) of parameters\n",
    "    :param X: n x m matrix of input features\n",
    "    :param Y: row vector (1 x m) of target values\n",
    "    :return: gradient at the current theta\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    grad = np.dot(np.dot(theta, X) - Y, X.T) / m\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_grad_validate(theta, X, Y, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    :param theta: row vector of parameters\n",
    "    :param X: n x m matrix of inputs\n",
    "    :param Y: row vector of target values\n",
    "    :param epsilon: real value << 0\n",
    "    :return: row vector gradient estimates for cost function at current theta\n",
    "    \"\"\"\n",
    "\n",
    "    n = theta.shape[1]\n",
    "    grad_validation = np.zeros(theta.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        theta_plus_epsilon = np.copy(theta)\n",
    "        theta_minus_epsilon = np.copy(theta)\n",
    "        theta_plus_epsilon[0, i] = theta[0, i] + epsilon\n",
    "        theta_minus_epsilon[0, i] = theta[0, i] - epsilon\n",
    "\n",
    "        cost_plus_epsilon = linreg_cost_from_inputs(theta_plus_epsilon, X, Y)\n",
    "        cost_minus_epsilon = linreg_cost_from_inputs(theta_minus_epsilon, X, Y)\n",
    "\n",
    "        grad_validation[0, i] = (cost_plus_epsilon - cost_minus_epsilon) / (2 * epsilon)\n",
    "\n",
    "    return grad_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_from_loss (shape):  (1, 2)\n",
      "[[-0.33333333 -2.66666667]] \n",
      "\n",
      "grad_from_inputs (shape):  (1, 2)\n",
      "[[-0.33333333 -2.66666667]] \n",
      "\n",
      "grad_validation (shape):  (1, 2)\n",
      "[[-0.33333333 -2.66666667]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linreguni-grad\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, :])\n",
    "y = np.array(linreg_y[:, :])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "\n",
    "y_hat = linreg_h_theta(theta, X)\n",
    "L_theta = linreg_loss_theta(y_hat, y)\n",
    "\n",
    "grad_from_loss = linreg_grad_from_loss(L_theta, X)\n",
    "print('grad_from_loss (shape): ', grad_from_loss.shape)\n",
    "print(grad_from_loss, '\\n')\n",
    "\n",
    "grad_from_inputs = linreg_grad_from_inputs(theta, X, y)\n",
    "print('grad_from_inputs (shape): ', grad_from_inputs.shape)\n",
    "print(grad_from_inputs, '\\n')\n",
    "\n",
    "grad_validation = linreg_grad_validate(theta, X, y)\n",
    "print('grad_validation (shape): ', grad_validation.shape)\n",
    "print(grad_validation, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_update_params(theta, grad, alpha):\n",
    "    \"\"\"\n",
    "    :param theta: row vector (1 x n) of current parameters\n",
    "    :param grad: row vector (1 x n) of gradients\n",
    "    :param alpha: learning rate as real number\n",
    "    :return: row vector of updated parmeters\n",
    "    \"\"\"\n",
    "\n",
    "    params = theta - alpha * grad\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_from_inputs (shape):  (1, 2)\n",
      "[[-0.33333333 -2.66666667]] \n",
      "\n",
      "theta (shape):  (1, 2)\n",
      "[[2. 5.]] \n",
      "\n",
      "params (shape):  (1, 2)\n",
      "[[2.00333333 5.02666667]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linreguni-param-update\n",
    "\n",
    "theta = np.array([[2., 5.]])\n",
    "x = np.array(linreg_x[:, :])\n",
    "y = np.array(linreg_y[:, :])\n",
    "x_bias = np.ones((1, x.shape[1]))\n",
    "X = np.vstack((x_bias, x))\n",
    "alpha = 0.01\n",
    "\n",
    "grad_from_inputs = linreg_grad_from_inputs(theta, X, y)\n",
    "print('grad_from_inputs (shape): ', grad_from_inputs.shape)\n",
    "print(grad_from_inputs, '\\n')\n",
    "\n",
    "params = linreg_update_params(theta, grad_from_inputs, alpha)\n",
    "print('theta (shape): ', theta.shape)\n",
    "print(theta, '\\n')\n",
    "print('params (shape): ', params.shape)\n",
    "print(params, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}